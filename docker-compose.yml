services:
  app:
    build: .
    image: assistant-ai:latest
    container_name: assistant-ai-app
    restart: unless-stopped
    environment:
      GRPC_ADDR: "0.0.0.0:50051"
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      # LLM API keys (set in .env file)
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      # For local models (Ollama, LocalAI, etc.)
      # macOS/Windows: use host.docker.internal instead of localhost
      # Example: http://host.docker.internal:11434/v1
      OPENAI_BASE_URL: ${OPENAI_BASE_URL:-}
      OPENAI_MODEL: ${OPENAI_MODEL:-gpt-3.5-turbo}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}
    ports:
      - "50052:50051"
    # Enable access to host machine from container
    # macOS/Windows: host.docker.internal is automatically available
    # Linux: may need to add extra_hosts: ["host.docker.internal:host-gateway"]
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - assistant-net

networks:
  # Reuse the network from assistant_account
  assistant-net:
    name: assistant_assistant-net
    external: true

